{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVOBTQ6gLHFKAr7dB8Z18T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndoorAlanD/DA6401-Assignment-1/blob/main/Question_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "_VsynOwyMEjW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi5p2ZYML7Mv",
        "outputId": "1e4bd7a8-9460-46cb-d1da-3d4a41cfb234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "#loading the dataset\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)\n",
        "class_names= ['T-shirt', 'Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "X_train = X_train.reshape(X_train.shape[0], 28*28) / 255\n",
        "X_test = X_test.reshape(X_test.shape[0], 28*28) /255\n",
        "X_val = X_val.reshape(X_val.shape[0], 28*28) /255"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Feed_Forward_Neural_Network():\n",
        "\n",
        "  def __init__(self, config):\n",
        "    self.theta = []\n",
        "    self.W = []\n",
        "    self.b = []\n",
        "\n",
        "    self.config = config\n",
        "    self.eta = self.config['lr']\n",
        "\n",
        "    self.l = 2\n",
        "    self.L = []\n",
        "    self.L.append(X_train.shape[1])\n",
        "\n",
        "\n",
        "    self.l += self.config['num_layers']\n",
        "    for i in range(self.l-2):\n",
        "      self.L.append(self.config['hidden_size'])\n",
        "    self.L.append(np.max(y_train)+1)\n",
        "    print(self.L)\n",
        "\n",
        "\n",
        "    for i in range(self.l-1):\n",
        "      self.W.append(np.random.randn(self.L[i+1], self.L[i]))\n",
        "      self.b.append(np.random.randn(self.L[i+1]))\n",
        "\n",
        "    if self.config['activation'] == 'sigmoid':\n",
        "      self.act = self.sigmoid\n",
        "      self.act_der = self.sigmoid_der\n",
        "    elif self.config['activation'] == 'ReLU':\n",
        "      self.act = self.relu\n",
        "      self.act_der = self.relu_der\n",
        "    else:\n",
        "      self.act = self.identity\n",
        "      self.act_der = self.identity_der\n",
        "\n",
        "    self.optimizer = self.classic_gd(X_train, y_train)\n",
        "\n",
        "\n",
        "  def identity(self, x):\n",
        "    return x\n",
        "\n",
        "  def identity_der(self, x):\n",
        "    return np.ones(len(x))\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "  def sigmoid_der(self, x):\n",
        "    sg = self.sigmoid(x)\n",
        "    return sg * (1 - sg)\n",
        "\n",
        "  def relu(self, x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "  def relu_der(self, x):\n",
        "    return (x > 0).astype(int)\n",
        "\n",
        "  def softmax(self, x):\n",
        "    new_x =np.exp(x - np.max(x))\n",
        "    # new_x = np.exp(np.clip(x,-500, 500))\n",
        "    return  new_x / new_x.sum()\n",
        "\n",
        "\n",
        "  # def data_log(self, X=X_train, y=y_train):\n",
        "  #   val_acc, val_true, val_pred = self.accuracy()\n",
        "  #   train_acc, train_true, train_pred = self.accuracy(X, y)\n",
        "  #   # val_acu=self.accuracy()\n",
        "  #   # train_acu=self.accuracy(X,y)\n",
        "  #   val_loss=self.cross_entropy_loss()\n",
        "  #   train_loss=self.cross_entropy_loss(X,y)\n",
        "  #   print(val_acu)\n",
        "  #   wandb.log({\n",
        "  #       'val_accuracy': val_acc,'val_loss': val_loss,'accuracy': train_acc,'loss': train_loss,\n",
        "  #       'confusion_matrix': wandb.plot.confusion_matrix(probs=None,\n",
        "  #                                                       y_true=val_true,\n",
        "  #                                                       preds=val_pred,\n",
        "  #                                                       cl ass_names=[str(i) for i in range(np.max(y) + 1)])\n",
        "  #   })\n",
        "  #   # wandb.log({'val_accuracy': val_acu,'val_loss': val_loss,'accuracy': train_acu,'loss': train_loss})\n",
        "\n",
        "  # # def accuracy(self, X=X_val, y=y_val):\n",
        "  # #   correct_predictions = sum(np.argmax(self.feed_forward(x)[-1]) == y_true for x, y_true in zip(X, y))\n",
        "  # #   return (correct_predictions / len(y)) * 100.0\n",
        "\n",
        "  # def accuracy(self, X=X_val, y=y_val):\n",
        "  #   y_pred_labels = []\n",
        "  #   y_true_labels = []\n",
        "\n",
        "  #   for x, y_true in zip(X, y):\n",
        "  #       y_pred = np.argmax(self.feed_forward(x)[-1])\n",
        "  #       y_pred_labels.append(y_pred)\n",
        "  #       y_true_labels.append(y_true)\n",
        "\n",
        "  #   correct_predictions = sum(y_p == y_t for y_p, y_t in zip(y_pred_labels, y_true_labels))\n",
        "  #   acc = (correct_predictions / len(y)) * 100.0\n",
        "\n",
        "  #   return acc, y_true_labels, y_pred_labels\n",
        "\n",
        "  def data_log(self, X=X_train, y=y_train):\n",
        "    val_acc, val_true, val_pred = self.accuracy()\n",
        "    train_acc, train_true, train_pred = self.accuracy(X, y)\n",
        "    val_loss = self.cross_entropy_loss()\n",
        "    train_loss = self.cross_entropy_loss(X, y)\n",
        "\n",
        "    print(\"Validation Accuracy:\", val_acc)\n",
        "    print(\"Validation Loss:\", val_loss)\n",
        "    print(\"Training Accuracy:\", train_acc)\n",
        "    print(\"Training Loss:\", train_loss)\n",
        "\n",
        "    # Print confusion matrix\n",
        "    num_classes = np.max(y) + 1\n",
        "    print(\"Confusion Matrix:\")\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
        "\n",
        "    for true, pred in zip(val_true, val_pred):\n",
        "      confusion_matrix[true, pred] += 1\n",
        "\n",
        "    print(confusion_matrix)\n",
        "\n",
        "  def accuracy(self, X=X_val, y=y_val):\n",
        "    y_pred_labels = []\n",
        "    y_true_labels = []\n",
        "\n",
        "    for x, y_true in zip(X, y):\n",
        "      y_pred = np.argmax(self.feed_forward(x)[-1])\n",
        "      y_pred_labels.append(y_pred)\n",
        "      y_true_labels.append(y_true)\n",
        "\n",
        "    correct_predictions = sum(y_p == y_t for y_p, y_t in zip(y_pred_labels, y_true_labels))\n",
        "    acc = (correct_predictions / len(y)) * 100.0\n",
        "\n",
        "    return acc, y_true_labels, y_pred_labels\n",
        "\n",
        "\n",
        "  def squared_loss(self, X=X_val, y=y_val):\n",
        "    sq_loss = 0\n",
        "    for (x_in, y_true) in zip(X, y):\n",
        "      _, _, y_out = self.feed_forward(x_in)\n",
        "      # y_out[y_true] -= 1\n",
        "      # sq_loss += np.sum(y_out**2)\n",
        "      y_diff = y_out.copy()  # Avoid modifying original `y_out`\n",
        "      y_diff[y_true] -= 1\n",
        "      sq_loss += np.sum(y_diff**2)\n",
        "    return sq_loss / len(y)\n",
        "\n",
        "  def cross_entropy_loss(self, X=X_val, y=y_val):\n",
        "    loss = 0\n",
        "    for (x_in, y_true) in zip(X, y):\n",
        "      a, h, y_out = self.feed_forward(x_in)\n",
        "      loss -= np.log(y_out[y_true] + 1e-10)\n",
        "    return loss / len(y)\n",
        "\n",
        "\n",
        "  def feed_forward(self, x):\n",
        "    pre_a = []\n",
        "    act_h = [x]\n",
        "\n",
        "    for i in range(self.l-2):\n",
        "      pre_a.append(np.dot(self.W[i], act_h[i]) + self.b[i])\n",
        "      act_h.append(self.act(pre_a[-1]))\n",
        "    pre_a.append(self.b[-1] + np.dot(self.W[-1], act_h[-1]))\n",
        "    y_pred = self.softmax(pre_a[-1])\n",
        "    return pre_a, act_h, y_pred\n",
        "\n",
        "  def back_prop(self, a, h, y, y_pred):\n",
        "    grad_a = []\n",
        "\n",
        "    if(self.config['loss'] == 'squared'):\n",
        "      y_p = y_pred * (1-y_pred)\n",
        "      y_pred[y] -= 1\n",
        "      grad_a = np.array(y_p * y_pred)\n",
        "    else:\n",
        "      for i in range(len(y_pred)):\n",
        "        if(i == y):\n",
        "          grad_a.append(y_pred[i]-1)\n",
        "        else:\n",
        "          grad_a.append(y_pred[i])\n",
        "      grad_a = np.array(grad_a)\n",
        "\n",
        "    i = self.l-1\n",
        "    grad_W, grad_b = [], []\n",
        "    while i>0:\n",
        "      grad_W.append(np.outer(grad_a, np.array(h[i-1])))\n",
        "      grad_b.append(grad_a)\n",
        "      if(i>1):\n",
        "        grad_h = np.matmul(self.W[i-1].T, grad_a)\n",
        "        grad_a = np.multiply(grad_h, self.act_der(a[i-2]))\n",
        "      i -= 1\n",
        "\n",
        "    return grad_W, grad_b\n",
        "\n",
        "  def d_init(self):\n",
        "    dw = [np.zeros_like(w) for w in self.W]\n",
        "    db = [np.zeros_like(bi) for bi in self.b]\n",
        "    return dw, db\n",
        "\n",
        "  def classic_gd(self, X=X_train, y=y_train):\n",
        "    max_epochs = self.config['epochs']\n",
        "    index = 0\n",
        "\n",
        "    for ep in range(max_epochs):\n",
        "      print(f\"Epoch {ep+1}/{max_epochs}\")\n",
        "      dw, db = self.d_init()\n",
        "\n",
        "      for (x, y_true) in zip(X, y):\n",
        "        a, h, y_pred = self.feed_forward(x)\n",
        "        grad_W, grad_b = self.back_prop(a, h, y_true, y_pred)\n",
        "\n",
        "        for i in range(self.l-1):\n",
        "          dw[i] += grad_W[-1-i]\n",
        "          db[i] += grad_b[-1-i]\n",
        "\n",
        "      for i in range(self.l-1):\n",
        "        self.W[i] -= self.eta*dw[i]\n",
        "        self.b[i] -= self.eta*db[i]\n",
        "      dw, db = self.d_init()\n",
        "\n",
        "      self.data_log()\n"
      ],
      "metadata": {
        "id": "GY_2uM9EMB82"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'lr': 0.01,\n",
        "    'num_layers': 3,\n",
        "    'hidden_size': 64,\n",
        "    'epochs': 5,\n",
        "    'activation': 'ReLU',\n",
        "    'loss': 'cross_entropy'\n",
        "}\n",
        "#cross_entropy\n",
        "#squared"
      ],
      "metadata": {
        "id": "HulxHfa7MB7f"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Feed_Forward_Neural_Network(config)\n",
        "model.classic_gd(X_train, y_train)"
      ],
      "metadata": {
        "id": "QKaON31rMB1O"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZV7O0FGIMBzq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}