{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/wQd2cM14V5L9moDYPMU1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndoorAlanD/DA6401-Assignment-1/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Lpk3fatdTBo",
        "outputId": "0543c458-025e-4e40-e06f-20441622361f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train.py\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import wandb\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class Feed_Forward_Neural_Network():\n",
        "\n",
        "  def __init__(self, config):\n",
        "    self.W = []\n",
        "    self.b = []\n",
        "\n",
        "    self.config = config\n",
        "    self.eta = self.config['lr']\n",
        "\n",
        "    self.max_ep=self.config['epochs']\n",
        "    self.b_s=self.config['batch_size']\n",
        "    self.w_de=self.config['weight_decay']\n",
        "    self.eps=self.config['epsilon']\n",
        "\n",
        "    self.X_train = config[\"X_train\"]\n",
        "    self.y_train = config[\"y_train\"]\n",
        "    self.X_val = config[\"X_val\"]\n",
        "    self.y_val = config[\"y_val\"]\n",
        "    self.X_test = config[\"X_test\"]\n",
        "    self.y_test = config[\"y_test\"]\n",
        "\n",
        "    self.l = 2\n",
        "    self.L = []\n",
        "    self.L.append(self.X_train.shape[1])\n",
        "\n",
        "    self.l += self.config['num_layers']\n",
        "    for i in range(self.l-2):\n",
        "      self.L.append(self.config['hidden_size'])\n",
        "    self.L.append(np.max(self.y_train)+1)\n",
        "\n",
        "    if(config['weight_init']=='random'):\n",
        "      for i in range(self.l-1):\n",
        "        self.W.append(np.random.randn(self.L[i+1], self.L[i]))\n",
        "        self.b.append(np.random.randn(self.L[i+1]))\n",
        "    else:\n",
        "      for i in range(self.l-1):\n",
        "        xav_std = np.sqrt(2 / (self.L[i+1] + self.L[i]))\n",
        "        self.W.append(np.random.randn(self.L[i+1], self.L[i]) * xav_std)\n",
        "        self.b.append(np.random.randn(self.L[i+1]))\n",
        "\n",
        "    if self.config['activation']=='sigmoid':\n",
        "      self.act =self.sigmoid\n",
        "      self.act_der =self.sigmoid_der\n",
        "    elif self.config['activation']=='tanh':\n",
        "      self.act =self.tanh\n",
        "      self.act_der =self.tanh_der\n",
        "    elif self.config['activation']=='ReLU':\n",
        "      self.act =self.relu\n",
        "      self.act_der =self.relu_der\n",
        "    else:\n",
        "      self.act =self.identity\n",
        "      self.act_der =self.identity_der\n",
        "\n",
        "    optimizers = {\n",
        "        'sgd': self.stochastic,\n",
        "        'momentum': self.momentum,\n",
        "        'nag': self.nesterov,\n",
        "        'rmsprop': self.rmsprop,\n",
        "        'adam': self.adam,\n",
        "        'nadam': self.nadam\n",
        "    }\n",
        "    self.optimizer = optimizers[self.config['optimizer']](self.X_train, self.y_train)\n",
        "\n",
        "\n",
        "  def identity(self, x):\n",
        "    return x\n",
        "\n",
        "  def identity_der(self, x):\n",
        "    return np.ones(len(x))\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "  def sigmoid_der(self, x):\n",
        "    sg = self.sigmoid(x)\n",
        "    return sg * (1 - sg)\n",
        "\n",
        "  def tanh(self, x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "  def tanh_der(self, x):\n",
        "    new_x = self.tanh(x)\n",
        "    return 1-(new_x*new_x)\n",
        "\n",
        "  def relu(self, x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "  def relu_der(self, x):\n",
        "    return (x > 0).astype(int)\n",
        "\n",
        "  def softmax(self, x):\n",
        "    new_x = np.exp(np.clip(x,-500, 500))\n",
        "    return  new_x / new_x.sum()\n",
        "\n",
        "  def log_metrics(self, X1=None, y1=None, X2=None, y2=None, X3=None, y3=None):\n",
        "    X1 = X1 if X1 is not None else self.X_train\n",
        "    y1 = y1 if y1 is not None else self.y_train\n",
        "    X2 = X2 if X2 is not None else self.X_val\n",
        "    y2 = y2 if y2 is not None else self.y_val\n",
        "    X3 = X3 if X3 is not None else self.X_test\n",
        "    y3 = y3 if y3 is not None else self.y_test\n",
        "    train_acc, train_loss = self.compute_accuracy_and_loss(X1, y1)\n",
        "    val_acc, val_loss = self.compute_accuracy_and_loss(X2, y2)\n",
        "    test_acc, test_loss = self.compute_accuracy_and_loss(X3, y3)\n",
        "    print(test_acc)\n",
        "\n",
        "    wandb.log({\n",
        "        'train_accuracy': train_acc,\n",
        "        'train_loss': train_loss,\n",
        "        'val_accuracy': val_acc,\n",
        "        'val_loss': val_loss,\n",
        "        'test_accuracy': test_acc,\n",
        "        'test_loss': test_loss\n",
        "    })\n",
        "\n",
        "  def log_confusion_matrix(self, X=None, y=None, dataset_name=\"Test\"):\n",
        "    X = X if X is not None else self.X_test\n",
        "    y = y if y is not None else self.y_test\n",
        "    y_true, y_pred = self.compute_predictions(X, y)\n",
        "    wandb.log({\n",
        "        f'{dataset_name}_confusion_matrix': wandb.plot.confusion_matrix(\n",
        "            probs=None,\n",
        "            y_true=y_true,\n",
        "            preds=y_pred,\n",
        "            class_names=[str(i) for i in range(np.max(y_true) + 1)]\n",
        "        )\n",
        "    })\n",
        "\n",
        "  def compute_accuracy_and_loss(self, X, y):\n",
        "    y_true, y_pred = self.compute_predictions(X, y)\n",
        "    correct_predictions = sum(y_p == y_t for y_p, y_t in zip(y_pred, y_true))\n",
        "    acc = (correct_predictions / len(y)) * 100.0\n",
        "    if(self.config['loss'] == 'squared'):\n",
        "      loss = self.squared_loss(X, y)\n",
        "    else:\n",
        "      loss = self.cross_entropy_loss(X, y)\n",
        "    return acc, loss\n",
        "\n",
        "  def compute_predictions(self, X, y):\n",
        "    y_pred_labels = [np.argmax(self.feed_forward(x)[-1]) for x in X]\n",
        "    return y, y_pred_labels\n",
        "\n",
        "  def squared_loss(self, X, y):\n",
        "    sq_loss = 0\n",
        "    for (x_in, y_true) in zip(X, y):\n",
        "      _, _, y_pr = self.feed_forward(x_in)\n",
        "      y_pr[y_true] -= 1\n",
        "      sq_loss += np.sum(y_pr**2)\n",
        "    return sq_loss / len(y)\n",
        "\n",
        "  def cross_entropy_loss(self, X, y):\n",
        "    loss = 0\n",
        "    for (x_in, y_true) in zip(X, y):\n",
        "      _, _, y_out = self.feed_forward(x_in)\n",
        "      loss -= np.log(y_out[y_true]+1e-10)\n",
        "    return loss / len(y)\n",
        "\n",
        "  def feed_forward(self, x, W=None, b=None):\n",
        "    W = W if W is not None else self.W\n",
        "    b = b if b is not None else self.b\n",
        "    pre_a = []\n",
        "    act_h = [x]\n",
        "\n",
        "    for i in range(self.l-2):\n",
        "      pre_a.append(np.dot(W[i], act_h[i]) + b[i])\n",
        "      act_h.append(self.act(pre_a[-1]))\n",
        "    pre_a.append(b[-1] + np.dot(W[-1], act_h[-1]))\n",
        "    y_pred = self.softmax(pre_a[-1])\n",
        "    return pre_a, act_h, y_pred\n",
        "\n",
        "  def back_prop(self, a, h, y, y_pred, W=None):\n",
        "    W = W if W is not None else self.W\n",
        "    gr_a = []\n",
        "\n",
        "    if(self.config['loss'] == 'mean_squared_error'):\n",
        "      y_p = y_pred * (1-y_pred)\n",
        "      y_pred[y] -= 1\n",
        "      gr_a = np.array(y_p * y_pred)\n",
        "    else:\n",
        "      for i in range(len(y_pred)):\n",
        "        if(i == y):\n",
        "          gr_a.append(y_pred[i]-1)\n",
        "        else:\n",
        "          gr_a.append(y_pred[i])\n",
        "      gr_a = np.array(gr_a)\n",
        "\n",
        "    i = self.l-1\n",
        "    gr_W, gr_b = [], []\n",
        "    while i>0:\n",
        "      gr_W.append(np.outer(gr_a, np.array(h[i-1])))\n",
        "      gr_b.append(gr_a)\n",
        "      if(i>1):\n",
        "        gr_h = np.matmul(W[i-1].T, gr_a)\n",
        "        gr_a = np.multiply(gr_h, self.act_der(a[i-2]))\n",
        "      i -= 1\n",
        "\n",
        "    return gr_W, gr_b\n",
        "\n",
        "  def d_init(self):\n",
        "    dw = [np.zeros_like(w) for w in self.W]\n",
        "    db = [np.zeros_like(bi) for bi in self.b]\n",
        "    return dw, db\n",
        "\n",
        "  def momentum(self, X, y):\n",
        "    be=self.config['momentum']\n",
        "    uw, ub = self.d_init()\n",
        "    ind=0\n",
        "\n",
        "    for ep in range(self.max_ep):\n",
        "      print(f\"Epoch {ep+1}/{self.max_ep}\")\n",
        "      dw, db = self.d_init()\n",
        "\n",
        "      for (x, y_true) in zip(X, y):\n",
        "        a, h, y_pre = self.feed_forward(x)\n",
        "        gr_W, gr_b = self.back_prop(a, h, y_true, y_pre)\n",
        "\n",
        "        for i in range(self.l-1):\n",
        "          dw[i] += gr_W[-1-i]\n",
        "          db[i] += gr_b[-1-i]\n",
        "\n",
        "        ind += 1\n",
        "        if(ind == self.b_s):\n",
        "          for i in range(self.l-1):\n",
        "            uw[i] =be*uw[i]+self.eta*dw[i]\n",
        "            ub[i] =be*ub[i]+self.eta*db[i]\n",
        "            self.W[i] -= uw[i]+self.eta*self.w_de*self.W[i]\n",
        "            self.b[i] -= ub[i]+self.eta*self.w_de*self.b[i]\n",
        "          dw, db = self.d_init()\n",
        "          ind=0\n",
        "\n",
        "      self.log_metrics()\n",
        "    self.log_confusion_matrix()\n",
        "\n",
        "  def nesterov(self, X, y):\n",
        "    be=self.config['momentum']\n",
        "    vw, vb = self.d_init()\n",
        "    ind = 0\n",
        "\n",
        "    for ep in range(self.max_ep):\n",
        "      print(f\"Epoch {ep+1}/{self.max_ep}\")\n",
        "      dw, db = self.d_init()\n",
        "\n",
        "      for i in range(self.l-1):\n",
        "        self.W[i] -= be*vw[i]\n",
        "        self.b[i] -= be*vb[i]\n",
        "\n",
        "      for (x, y_true) in zip(X, y):\n",
        "        a, h, y_pre = self.feed_forward(x)\n",
        "        gr_W, gr_b = self.back_prop(a, h, y_true, y_pre)\n",
        "\n",
        "        for i in range(self.l-1):\n",
        "          dw[i] += gr_W[-1-i]\n",
        "          db[i] += gr_b[-1-i]\n",
        "\n",
        "        ind += 1\n",
        "        if(ind == self.b_s):\n",
        "          for i in range(self.l-1):\n",
        "            dw[i] += self.w_de*self.W[i]\n",
        "            vw[i] = be*vw[i] + self.eta*dw[i]\n",
        "            vb[i] = be*vb[i] + self.eta*db[i]\n",
        "            self.W[i] -= self.eta*dw[i]\n",
        "            self.b[i] -= self.eta*db[i]\n",
        "          dw, db = self.d_init()\n",
        "          ind=0\n",
        "\n",
        "      self.log_metrics()\n",
        "    self.log_confusion_matrix()\n",
        "\n",
        "  def stochastic(self, X, y):\n",
        "    ind=0\n",
        "\n",
        "    for ep in range(self.max_ep):\n",
        "      print(f\"Epoch {ep+1}/{self.max_ep}\")\n",
        "      dw, db = self.d_init()\n",
        "\n",
        "      for (x, y_true) in zip(X, y):\n",
        "        a, h, y_pre = self.feed_forward(x)\n",
        "        gr_W, gr_b = self.back_prop(a, h, y_true, y_pre)\n",
        "\n",
        "        for i in range(self.l-1):\n",
        "          dw[i] += gr_W[-1-i]\n",
        "          db[i] += gr_b[-1-i]\n",
        "\n",
        "        ind += 1\n",
        "        if(ind % self.b_s == 0):\n",
        "          for i in range(len(self.W)):\n",
        "            dw[i] += self.w_de*self.W[i]\n",
        "            self.W[i] -= self.eta*np.array(dw[i])\n",
        "            self.b[i] -= self.eta*np.array(db[i])\n",
        "          dw, db = self.d_init()\n",
        "          ind=0\n",
        "\n",
        "      self.log_metrics()\n",
        "    self.log_confusion_matrix()\n",
        "\n",
        "\n",
        "  def rmsprop(self, X, y):\n",
        "    be=self.config['beta']\n",
        "    be=0.9\n",
        "    vw, vb = self.d_init()\n",
        "    ind=0\n",
        "\n",
        "    for ep in range(self.max_ep):\n",
        "      print(f\"Epoch {ep+1}/{self.max_ep}\")\n",
        "      dw, db = self.d_init()\n",
        "\n",
        "      for (x, y_true) in zip(X, y):\n",
        "        a, h, y_pre = self.feed_forward(x)\n",
        "        gr_W, gr_b = self.back_prop(a, h, y_true, y_pre)\n",
        "        for i in range(self.l-1):\n",
        "          dw[i] += gr_W[-1-i]\n",
        "          db[i] += gr_b[-1-i]\n",
        "\n",
        "        ind += 1\n",
        "        if(ind == self.b_s):\n",
        "          for i in range(self.l-1):\n",
        "            dw[i] += self.w_de*self.W[i]\n",
        "            vw[i]=be*vw[i]+(1-be)*(dw[i]**2)\n",
        "            vb[i]=be*vb[i]+(1-be)*(db[i]**2)\n",
        "            self.W[i] -= self.eta*dw[i]/(np.sqrt(vw[i])+self.eps)\n",
        "            self.b[i] -= self.eta*db[i]/(np.sqrt(vb[i])+self.eps)\n",
        "          dw, db = self.d_init()\n",
        "          ind=0\n",
        "\n",
        "      self.log_metrics()\n",
        "    self.log_confusion_matrix()\n",
        "\n",
        "  def adam(self, X, y):\n",
        "    b1=self.config['beta1']\n",
        "    b2=self.config['beta2']\n",
        "    ind= 0\n",
        "\n",
        "    mw, mb = self.d_init()\n",
        "    vw, vb = self.d_init()\n",
        "    mw_t, mb_t = self.d_init()\n",
        "    vw_t, vb_t = self.d_init()\n",
        "\n",
        "    for ep in range(self.max_ep):\n",
        "      print(f\"Epoch {ep+1}/{self.max_ep}\")\n",
        "      dw, db = self.d_init()\n",
        "\n",
        "      for (x, y_true) in zip(X, y):\n",
        "        a, h, y_pre = self.feed_forward(x)\n",
        "        gr_W, gr_b = self.back_prop(a, h, y_true, y_pre)\n",
        "        for i in range(self.l-1):\n",
        "            dw[i] += gr_W[-1-i]\n",
        "            db[i] += gr_b[-1-i]\n",
        "\n",
        "        ind += 1\n",
        "        if(ind == self.b_s):\n",
        "          for i in range(self.l-1):\n",
        "            dw[i] += self.w_de*self.W[i]\n",
        "            mw[i]=b1*mw[i]+(1-b1)*dw[i]\n",
        "            mb[i]=b1*mb[i]+(1-b1)*db[i]\n",
        "            vw[i]=b2*vw[i]+(1-b2)*dw[i]**2\n",
        "            vb[i]=b2*vb[i]+(1-b2)*db[i]**2\n",
        "            mw_t[i]=mw[i]/(1-np.power(b1, ep+1))\n",
        "            mb_t[i]=mb[i]/(1-np.power(b1, ep+1))\n",
        "            vw_t[i]=vw[i]/(1-np.power(b2, ep+1))\n",
        "            vb_t[i]=vb[i]/(1-np.power(b2, ep+1))\n",
        "            self.W[i] -= self.eta*mw_t[i]/(np.sqrt(vw_t[i])+self.eps)\n",
        "            self.b[i] -= self.eta*mb_t[i]/(np.sqrt(vb_t[i])+self.eps)\n",
        "          dw, db = self.d_init()\n",
        "          ind=0\n",
        "\n",
        "      self.log_metrics()\n",
        "    self.log_confusion_matrix()\n",
        "\n",
        "  def nadam(self, X, y):\n",
        "    b1=self.config['beta1']\n",
        "    b2=self.config['beta2']\n",
        "    mw, mb = self.d_init()\n",
        "    vw, vb = self.d_init()\n",
        "    mw_t, mb_t = self.d_init()\n",
        "    vw_t, vb_t = self.d_init()\n",
        "    ind= 0\n",
        "\n",
        "    for ep in range(self.max_ep):\n",
        "      print(f\"Epoch {ep+1}/{self.max_ep}\")\n",
        "      dw, db = self.d_init()\n",
        "\n",
        "      for (x, y_true) in zip(X, y):\n",
        "        a, h, y_pre = self.feed_forward(x)\n",
        "        gr_W, gr_b = self.back_prop(a, h, y_true, y_pre)\n",
        "        for i in range(self.l-1):\n",
        "            dw[i] += gr_W[-1-i]\n",
        "            db[i] += gr_b[-1-i]\n",
        "\n",
        "        ind += 1\n",
        "        if(ind == self.b_s):\n",
        "          for i in range(self.l-1):\n",
        "            dw[i] += self.w_de*self.W[i]\n",
        "            mw[i]=b1*mw[i]+(1-b1)*dw[i]\n",
        "            mb[i]=b1*mb[i]+(1-b1)*db[i]\n",
        "            vw[i]=b2*vw[i]+(1-b2)*dw[i]**2\n",
        "            vb[i]=b2*vb[i]+(1-b2)*db[i]**2\n",
        "            mw_t[i]=mw[i]/(1-np.power(b1, ep+1))\n",
        "            mb_t[i]=mb[i]/(1-np.power(b1, ep+1))\n",
        "            vw_t[i]=vw[i]/(1-np.power(b2, ep+1))\n",
        "            vb_t[i]=vb[i]/(1-np.power(b2, ep+1))\n",
        "            self.W[i] -= (self.eta/(np.sqrt(vw_t[i])+self.eps)) * (b1*mw_t[i] + (1-b1)*dw[i]/(1-b1**(ep+1)))\n",
        "            self.b[i] -= (self.eta/(np.sqrt(vb_t[i])+self.eps)) * (b1*mb_t[i] + (1-b1)*db[i]/(1-b1**(ep+1)))\n",
        "          dw, db = self.d_init()\n",
        "          ind=0\n",
        "\n",
        "      self.log_metrics()\n",
        "    self.log_confusion_matrix()\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Train a Feed Forward Neural Network.\")\n",
        "\n",
        "    parser.add_argument(\"-wp\", \"--wandb_project\", type=str, default=\"DL_Assignment_1\")\n",
        "    parser.add_argument(\"-we\", \"--wandb_entity\", type=str, default=\"Alan_D_Andoor\")\n",
        "    parser.add_argument(\"-d\", \"--dataset\", type=str, default=\"fashion_mnist\", choices=[\"mnist\", \"fashion_mnist\"],)\n",
        "    parser.add_argument(\"-e\", \"--epochs\", type=int, default=10)\n",
        "    parser.add_argument(\"-b\", \"--batch_size\", type=int, default=64)\n",
        "    parser.add_argument(\"-l\", \"--loss\", type=str, default=\"cross_entropy\", choices=[\"mean_squared_error\", \"cross_entropy\"])\n",
        "    parser.add_argument(\"-o\", \"--optimizer\", type=str, default=\"nadam\",choices=[\"sgd\", \"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"])\n",
        "    parser.add_argument(\"-lr\", \"--learning_rate\", type=float, default=0.001)\n",
        "    parser.add_argument(\"-m\", \"--momentum\", type=float, default=0.99)\n",
        "    parser.add_argument(\"-beta\", \"--beta\", type=float, default=0.9)\n",
        "    parser.add_argument(\"-beta1\", \"--beta1\", type=float, default=0.9)\n",
        "    parser.add_argument(\"-beta2\", \"--beta2\", type=float, default=0.99)\n",
        "    parser.add_argument(\"-eps\", \"--epsilon\", type=float, default=0.0000001)\n",
        "    parser.add_argument(\"-w_d\", \"--weight_decay\", type=float, default=0.0005)\n",
        "    parser.add_argument(\"-w_i\", \"--weight_init\", type=str, default=\"Xavier\", choices=[\"random\", \"Xavier\"])\n",
        "    parser.add_argument(\"-nhl\", \"--num_layers\", type=int, default=5)\n",
        "    parser.add_argument(\"-sz\", \"--hidden_size\", type=int, default=64)\n",
        "    parser.add_argument(\"-a\", \"--activation\", type=str, default=\"tanh\", choices=[\"identity\", \"sigmoid\", \"tanh\", \"ReLU\"])\n",
        "\n",
        "    return vars(parser.parse_args())\n",
        "\n",
        "def main():\n",
        "  args = parse_args()\n",
        "\n",
        "  wandb.init(project=args[\"wandb_project\"], entity=args[\"wandb_entity\"], config=args)\n",
        "\n",
        "  if args[\"dataset\"] == \"mnist\":\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "  else:\n",
        "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)\n",
        "\n",
        "  X_train = X_train.reshape(X_train.shape[0], 28*28) / 255\n",
        "  X_test = X_test.reshape(X_test.shape[0], 28*28) /255\n",
        "  X_val = X_val.reshape(X_val.shape[0], 28*28) /255\n",
        "\n",
        "  config = {\n",
        "    \"lr\": args[\"learning_rate\"],\n",
        "    \"epochs\": args[\"epochs\"],\n",
        "    \"batch_size\": args[\"batch_size\"],\n",
        "    \"loss\": args[\"loss\"],\n",
        "    \"optimizer\": args[\"optimizer\"],\n",
        "    \"momentum\": args[\"momentum\"],\n",
        "    \"beta\": args[\"beta\"],\n",
        "    \"beta1\": args[\"beta1\"],\n",
        "    \"beta2\": args[\"beta2\"],\n",
        "    \"epsilon\": args[\"epsilon\"],\n",
        "    \"weight_decay\": args[\"weight_decay\"],\n",
        "    \"weight_init\": args[\"weight_init\"],\n",
        "    \"num_layers\": args[\"num_layers\"],\n",
        "    \"hidden_size\": args[\"hidden_size\"],\n",
        "    \"activation\": args[\"activation\"],\n",
        "    \"X_train\": X_train,\n",
        "    \"y_train\": y_train,\n",
        "    \"X_val\": X_val,\n",
        "    \"y_val\": y_val,\n",
        "    \"X_test\": X_test,\n",
        "    \"y_test\": y_test\n",
        "  }\n",
        "\n",
        "  wandb.run.name = (\n",
        "        f\"{config['epochs']}_{config['optimizer']}_{config['activation']}_{config['loss']}_{config['batch_size']}_{config['num_layers']}_\"\n",
        "        f\"{config['lr']}_{config['weight_init']}\"\n",
        "    )\n",
        "\n",
        "  nn = Feed_Forward_Neural_Network(config)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --wandb_entity alandandoor-iit-madras --wandb_project DL_A1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcRxYRW2rH0s",
        "outputId": "a98d6c6d-2297-4e58-bffa-52d1f7ba02bd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-17 15:37:05.079322: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742225825.107177    8857 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742225825.116039    8857 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-17 15:37:05.144215: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malandandoor\u001b[0m (\u001b[33malandandoor-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250317_153709-j1yhe0fi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meager-breeze-898\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/alandandoor-iit-madras/DL_A1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/alandandoor-iit-madras/DL_A1/runs/j1yhe0fi\u001b[0m\n",
            "Epoch 1/10\n",
            "84.52\n",
            "Epoch 2/10\n",
            "86.13\n",
            "Epoch 3/10\n",
            "87.0\n",
            "Epoch 4/10\n",
            "87.32\n",
            "Epoch 5/10\n",
            "87.31\n",
            "Epoch 6/10\n",
            "87.59\n",
            "Epoch 7/10\n",
            "87.78\n",
            "Epoch 8/10\n",
            "87.78\n",
            "Epoch 9/10\n",
            "87.75\n",
            "Epoch 10/10\n",
            "87.79\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33meager-breeze-898\u001b[0m at: \u001b[34mhttps://wandb.ai/alandandoor-iit-madras/DL_A1/runs/j1yhe0fi\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250317_153709-j1yhe0fi/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --wandb_entity alandandoor-iit-madras --wandb_project DL_A1 -o adam -w_i random -a ReLU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb79LzWpwQLf",
        "outputId": "a985b53c-308f-4776-d2c7-d94e89e4978e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-17 16:14:36.646013: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742228076.673968   17906 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742228076.681727   17906 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-17 16:14:36.708440: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malandandoor\u001b[0m (\u001b[33malandandoor-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250317_161441-t44dmz7i\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmorning-bush-901\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/alandandoor-iit-madras/DL_A1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/alandandoor-iit-madras/DL_A1/runs/t44dmz7i\u001b[0m\n",
            "Epoch 1/10\n",
            "69.91000000000001\n",
            "Epoch 2/10\n",
            "73.76\n",
            "Epoch 3/10\n",
            "74.82\n",
            "Epoch 4/10\n",
            "75.68\n",
            "Epoch 5/10\n",
            "76.88000000000001\n",
            "Epoch 6/10\n",
            "77.27000000000001\n",
            "Epoch 7/10\n",
            "78.05\n",
            "Epoch 8/10\n",
            "78.31\n",
            "Epoch 9/10\n",
            "78.68\n",
            "Epoch 10/10\n",
            "79.11\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mmorning-bush-901\u001b[0m at: \u001b[34mhttps://wandb.ai/alandandoor-iit-madras/DL_A1/runs/t44dmz7i\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250317_161441-t44dmz7i/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --wandb_entity alandandoor-iit-madras --wandb_project DL_A1 -o rmsprop -l mean_squared_error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGHkna0AwowS",
        "outputId": "dde5c50e-686c-489a-ecfa-13b524799c83"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-17 15:57:45.318500: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742227065.345212   13849 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742227065.353504   13849 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-17 15:57:45.379557: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malandandoor\u001b[0m (\u001b[33malandandoor-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250317_155750-2la7bg4s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstilted-moon-900\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/alandandoor-iit-madras/DL_A1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/alandandoor-iit-madras/DL_A1/runs/2la7bg4s\u001b[0m\n",
            "Epoch 1/10\n",
            "83.86\n",
            "Epoch 2/10\n",
            "84.89999999999999\n",
            "Epoch 3/10\n",
            "85.35000000000001\n",
            "Epoch 4/10\n",
            "86.61\n",
            "Epoch 5/10\n",
            "85.14\n",
            "Epoch 6/10\n",
            "85.33\n",
            "Epoch 7/10\n",
            "86.05000000000001\n",
            "Epoch 8/10\n",
            "86.91\n",
            "Epoch 9/10\n",
            "85.68\n",
            "Epoch 10/10\n",
            "85.96000000000001\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mstilted-moon-900\u001b[0m at: \u001b[34mhttps://wandb.ai/alandandoor-iit-madras/DL_A1/runs/2la7bg4s\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250317_155750-2la7bg4s/logs\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}